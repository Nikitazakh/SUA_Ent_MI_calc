{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d2ed49d",
   "metadata": {},
   "source": [
    "# Extracting spike trains from nex to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86498d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu as mw\n",
    "from scipy.stats import sem\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "import random\n",
    "\n",
    "import math\n",
    "\n",
    "import EntropyHub as EH\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import neo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae8a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting only background activity\n",
    "\n",
    "\n",
    "def several_spt_and_fons(fon_times, fon_durations, spt_times):\n",
    "    new_spt = []\n",
    "    new_isi_list = []\n",
    "    prev_fon_time = 0\n",
    "    prev_fon_duration = 0\n",
    "    prev_fon_duration_list = []\n",
    "\n",
    "    for fon_time, fon_duration in zip(fon_times, fon_durations):\n",
    "        # print('fon begins:',fon_time)\n",
    "        # print('fon end:',fon_time+fon_duration)\n",
    "        int_time = float(fon_time) - sum(\n",
    "            prev_fon_duration_list\n",
    "        )  # interval for shifting spikes to the left\n",
    "        for spt_time in spt_times:\n",
    "\n",
    "            if spt_time >= fon_time and spt_time <= fon_time + fon_duration:\n",
    "\n",
    "                new_spt.append(\n",
    "                    float(spt_time) - int_time\n",
    "                )  # add a left shifted spike to the result\n",
    "\n",
    "        # save the previous background for processing in the next cycle\n",
    "        prev_fon_time = float(fon_time)\n",
    "        prev_fon_duration = float(fon_duration)\n",
    "        prev_fon_duration_list.append(prev_fon_duration)\n",
    "\n",
    "    new_spt = np.sort(drop_sim_spikes(new_spt))\n",
    "    new_isi_list = [(new_spt[i] - new_spt[i - 1]) for i in range(1, len(new_spt))]\n",
    "\n",
    "    return new_spt, new_isi_list\n",
    "\n",
    "\n",
    "def drop_sim_spikes(spt_times):\n",
    "    return list(set(spt_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad48a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_dict = {\n",
    "    \"patient\": [],\n",
    "    \"doc_name\": [],\n",
    "    \"data_name\": [],\n",
    "    \"interval_name\": [],\n",
    "    \"spt_list\": [],\n",
    "    \"isi_list\": [],\n",
    "}\n",
    "\n",
    "pat_path = r\"./path/to/folder/with/nex/files\"\n",
    "\n",
    "ISI_total = []\n",
    "ISI_change_total = []\n",
    "ISI_rel_change_total = []\n",
    "\n",
    "\n",
    "spike_c = 0\n",
    "\n",
    "# patients ID list\n",
    "pat_list = listdir(pat_path)\n",
    "\n",
    "counter = 0\n",
    "c = 0\n",
    "# patients processing\n",
    "for patient in pat_list:\n",
    "    print(\"Patient processing:\", patient)\n",
    "\n",
    "    temp_c = 0\n",
    "\n",
    "    start = time.time()\n",
    "    folder_path = pat_path + patient\n",
    "    onlyfiles = [f for f in listdir(folder_path) if isfile(join(folder_path, f))]\n",
    "    # print(onlyfiles)\n",
    "    c += len(onlyfiles)\n",
    "    for f_name in onlyfiles:\n",
    "        if f_name[-4:] == \".nex\":\n",
    "            nex_reader = neo.io.NeuroExplorerIO(\n",
    "                filename=folder_path + \"/\" + str(f_name)\n",
    "            )\n",
    "            data = nex_reader.read()\n",
    "            num_of_spt = len(data[0].segments[0].spiketrains)\n",
    "            num_of_fons = len(data[0].segments[0].epochs)\n",
    "            spt_names = []\n",
    "            for el in data[0].segments[0].spiketrains:\n",
    "                spt_names.append(el.name)\n",
    "            fon_names = []\n",
    "            for el in data[0].segments[0].epochs:\n",
    "                fon_names.append(el.name)\n",
    "\n",
    "            # fon = \"AllFile\" AND ONLY!\n",
    "\n",
    "            if (\n",
    "                num_of_fons == 1\n",
    "                and str(data[0].segments[0].epochs[0].name) == \"AllFile\"\n",
    "            ):\n",
    "                fon_to_work = data[0].segments[0].epochs[0]\n",
    "                for spt in data[0].segments[0].spiketrains:\n",
    "                    spt_times, isi_list = several_spt_and_fons(\n",
    "                        list(el.times), list(el.durations), list(spt.times)\n",
    "                    )\n",
    "                    df_dict[\"patient\"].append(patient)\n",
    "                    df_dict[\"doc_name\"].append(f_name)\n",
    "                    df_dict[\"data_name\"].append(spt.name)\n",
    "                    df_dict[\"interval_name\"].append(fon_to_work.name)\n",
    "                    df_dict[\"spt_list\"].append(spt_times)\n",
    "                    df_dict[\"isi_list\"].append(isi_list)\n",
    "\n",
    "                    spike_c += 1\n",
    "                    temp_c += 1\n",
    "\n",
    "            # multiple backgrounds in file\n",
    "\n",
    "            if num_of_fons > 1:\n",
    "\n",
    "                # fon = \"fon\" AND ONLY THAT!\n",
    "\n",
    "                if \"fon\" in fon_names:\n",
    "                    # looking for which of the backgrounds has the name \"fon\"\n",
    "                    for fon_ind in range(len(data[0].segments[0].epochs)):\n",
    "                        if data[0].segments[0].epochs[fon_ind].name == \"fon\":\n",
    "                            fon_to_work = (\n",
    "                                data[0].segments[0].epochs[fon_ind]\n",
    "                            )  # selected background with name \"fon\"\n",
    "                            for spt in (\n",
    "                                data[0].segments[0].spiketrains\n",
    "                            ):  # put all spt on fon\n",
    "                                spt_times, isi_list = several_spt_and_fons(\n",
    "                                    list(fon_to_work.times),\n",
    "                                    list(fon_to_work.durations),\n",
    "                                    list(spt.times),\n",
    "                                )\n",
    "                                df_dict[\"patient\"].append(patient)\n",
    "                                df_dict[\"doc_name\"].append(f_name)\n",
    "                                df_dict[\"data_name\"].append(spt.name)\n",
    "                                df_dict[\"interval_name\"].append(fon_to_work.name)\n",
    "                                df_dict[\"spt_list\"].append(spt_times)\n",
    "                                df_dict[\"isi_list\"].append(isi_list)\n",
    "\n",
    "                                spike_c += 1\n",
    "                                temp_c += 1\n",
    "\n",
    "                # fon = \"AllFile\" and spt.name\n",
    "\n",
    "                elif \"fon\" not in fon_names:\n",
    "                    for spt in data[0].segments[0].spiketrains:\n",
    "                        if (\n",
    "                            \"fon_\" + str(spt.name) in fon_names\n",
    "                        ):  # check if there is a suitable fon for spt\n",
    "                            for el in (\n",
    "                                data[0].segments[0].epochs\n",
    "                            ):  # find a suitable fon for spt\n",
    "                                if str(el.name) == \"fon_\" + str(spt.name):\n",
    "                                    fon_to_work = el\n",
    "                                    spt_times, isi_list = several_spt_and_fons(\n",
    "                                        list(fon_to_work.times),\n",
    "                                        list(fon_to_work.durations),\n",
    "                                        list(spt.times),\n",
    "                                    )\n",
    "                                    df_dict[\"patient\"].append(patient)\n",
    "                                    df_dict[\"doc_name\"].append(f_name)\n",
    "                                    df_dict[\"data_name\"].append(spt.name)\n",
    "                                    df_dict[\"interval_name\"].append(fon_to_work.name)\n",
    "                                    df_dict[\"spt_list\"].append(spt_times)\n",
    "                                    df_dict[\"isi_list\"].append(isi_list)\n",
    "\n",
    "                                    spike_c += 1\n",
    "                                    temp_c += 1\n",
    "\n",
    "                        else:  # put the remaining spt on AllFile\n",
    "                            fon_to_work = data[0].segments[0].epochs[0]\n",
    "                            spt_times, isi_list = several_spt_and_fons(\n",
    "                                list(fon_to_work.times),\n",
    "                                list(fon_to_work.durations),\n",
    "                                list(spt.times),\n",
    "                            )\n",
    "                            df_dict[\"patient\"].append(patient)\n",
    "                            df_dict[\"doc_name\"].append(f_name)\n",
    "                            df_dict[\"data_name\"].append(spt.name)\n",
    "                            df_dict[\"interval_name\"].append(fon_to_work.name)\n",
    "                            df_dict[\"spt_list\"].append(spt_times)\n",
    "                            df_dict[\"isi_list\"].append(isi_list)\n",
    "\n",
    "                            spike_c += 1\n",
    "                            temp_c += 1\n",
    "\n",
    "                else:\n",
    "                    print(\"Unaccounted files\")\n",
    "                    print(patient, f_name)\n",
    "                    print()\n",
    "    print(\"    \", temp_c)\n",
    "\n",
    "\n",
    "print(\"Patient processing is over\")\n",
    "print(\"Spiketrains count\", spike_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b30fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spt_isi = pd.DataFrame(df_dict)\n",
    "df_spt_isi.to_csv('spt_isi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bedb316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988ca36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
