{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe94935c",
   "metadata": {},
   "source": [
    "# Entropy calculation for encoded spike trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu as mw\n",
    "from scipy.stats import sem\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "import random\n",
    "\n",
    "import math\n",
    "\n",
    "import EntropyHub as EH\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import neo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44c4122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('encoded_isi_v3.csv') #open file created with 2 CSV to ENCODED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e36c68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for counting number of subsets in list\n",
    "\n",
    "def counter_1(base, find):\n",
    "    max_base = len(base) - len(find) + 1\n",
    "    result = 0\n",
    "    for i in range(max_base):\n",
    "        yes = True\n",
    "        for j in range(len(find)):\n",
    "            if base[i + j] != find[j]:\n",
    "                yes = False\n",
    "                break\n",
    "        if yes:\n",
    "            result += 1\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af766605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy calculation\n",
    "\n",
    "\n",
    "def CAE_entropy(isi_change_list, word_len=2):\n",
    "    N = len(isi_change_list) - word_len + 1\n",
    "\n",
    "    rel_freq = {}  # Здесь хранится вероятность встретить каждый символ\n",
    "    counts_list = []\n",
    "    for i in range(0, N):\n",
    "        word = tuple(isi_change_list[i : i + word_len])\n",
    "        if word in rel_freq.keys():\n",
    "            pass\n",
    "        else:\n",
    "            counts = counter_1(isi_change_list, word)\n",
    "            counts_list.append(counts)\n",
    "            rel_freq[word] = counts / N\n",
    "    counts_list = np.array(counts_list)\n",
    "    counts_list = counts_list[counts_list > 0]\n",
    "    n = np.sum(counts_list)\n",
    "    p = counts_list / n\n",
    "\n",
    "    f1 = np.count_nonzero(counts_list == 1)\n",
    "    if f1 == n:\n",
    "        f1 = n - 1\n",
    "\n",
    "    C = 1 - f1 / n\n",
    "    pa = C * p\n",
    "    la = 1 - (1 - pa) ** n\n",
    "\n",
    "    return -np.sum(pa * np.log2(pa) / la), -np.sum(p * np.log2(p))  # CAE, ENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional entropy calculation\n",
    "\n",
    "def cond_ent(isi_list, samp_isi_list, word_len=2):\n",
    "    N = len(isi_list) - word_len + 1\n",
    "    M = len(samp_isi_list) - word_len + 1\n",
    "    if M != N:\n",
    "        return print(\"ERROR\")\n",
    "    r_prob = {}  # Здесь хранится вероятность встретить каждый символ\n",
    "    trial = {}\n",
    "    s_prob = {}\n",
    "    trial_list = []\n",
    "    for i in range(0, N):\n",
    "        word_r = tuple(isi_list[i : i + word_len])\n",
    "        word_s = tuple(samp_isi_list[i : i + word_len])\n",
    "        if word_r in r_prob.keys():\n",
    "            pass\n",
    "        else:\n",
    "            counts = counter_1(isi_list, word_r)\n",
    "            r_prob[word_r] = counts / N\n",
    "        if word_s in trial.keys():\n",
    "            trial[word_s].append(word_r)\n",
    "        else:\n",
    "            trial[word_s] = []\n",
    "            trial[word_s].append(word_r)\n",
    "            counts = counter_1(samp_isi_list, word_s)\n",
    "            s_prob[word_s] = counts / M\n",
    "    h_list = []\n",
    "    for word_s in s_prob.keys():\n",
    "        unique = set(trial[word_s])\n",
    "        for el in unique:\n",
    "            cond_prob = trial[word_s].count(el) / len(trial[word_s])\n",
    "            h_list.append(-s_prob[word_s] * cond_prob * np.log2(cond_prob))\n",
    "    return np.sum(h_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa33c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[\"isitisi\"][0].strip(\"][\").split(\", \")\n",
    "data = [float(j) for j in data]\n",
    "print(len(data))\n",
    "CAE_entropy(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8af11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ['sib', 'isitm', 'isitisi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f435b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_dict = {}\n",
    "low = 1\n",
    "top = 2\n",
    "for word_length in range(low, top + 1):\n",
    "    for param in params:\n",
    "        print(param)\n",
    "        ent_param_name = \"ent_\" + param + \"_wl\" + str(word_length)\n",
    "        cond_ent_param_name = \"cond_ent_\" + param + \"_wl\" + str(word_length)\n",
    "        shuf_ent_param_name = \"shuf_ent_\" + param + \"_wl\" + str(word_length)\n",
    "\n",
    "        ent_dict[ent_param_name] = []\n",
    "        ent_dict[cond_ent_param_name] = []\n",
    "        ent_dict[shuf_ent_param_name] = []\n",
    "\n",
    "        for i in range(len(df)):\n",
    "\n",
    "            data = df[param][i].strip(\"][\").split(\", \")\n",
    "            data = [float(j) for j in data]\n",
    "\n",
    "            res = CAE_entropy(data, word_len=word_length)\n",
    "\n",
    "            ent_dict[ent_param_name].append(res[1])\n",
    "\n",
    "            temp_res_cond_ent = []\n",
    "            temp_res_ent = []\n",
    "\n",
    "            for m in range(100):\n",
    "\n",
    "                samp_data = random.sample(data, k=len(data))\n",
    "\n",
    "                # calculating conditional entropy with shuffled signal\n",
    "                temp_res_cond = cond_ent(data, samp_data, word_len=word_length)\n",
    "                temp_res_cond_ent.append(temp_res_cond)\n",
    "\n",
    "                # calculation entropy of shuffled signal\n",
    "                temp_res = CAE_entropy(samp_data, word_len=word_length)\n",
    "                temp_res_ent.append(temp_res[1])\n",
    "\n",
    "            ent_dict[cond_ent_param_name].append(np.median(temp_res_cond_ent))\n",
    "            ent_dict[shuf_ent_param_name].append(np.median(temp_res_ent))\n",
    "    for key in ent_dict.keys():\n",
    "        df[key] = ent_dict[key]\n",
    "    df.to_csv(\"encoded_data_with_ents_wl\" + str(word_length) + \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83029cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ent_dict.keys():\n",
    "    df[key]=ent_dict[key]\n",
    "df.to_csv('encoded_data_with_ents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea5cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
